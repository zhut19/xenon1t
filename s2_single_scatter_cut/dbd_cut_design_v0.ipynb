{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, sys, time\n",
    "import numpy as np\n",
    "from numpy import sqrt, exp, pi, square\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None        # default='warn'\n",
    "import matplotlib\n",
    "from matplotlib.colors import LogNorm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'   # enable if you have a retina display\n",
    "from scipy.optimize import curve_fit, minimize\n",
    "from scipy.interpolate import interp1d, UnivariateSpline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from multihist import Histdd, Hist1d\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "from contextlib import contextmanager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Plotting ####\n",
    "params = {\n",
    "    'backend': 'Agg',\n",
    "    # colormap\n",
    "    'image.cmap' : 'viridis',\n",
    "    # figure\n",
    "    'figure.figsize' : (4, 2),\n",
    "    'font.size' : 32,\n",
    "    'font.family' : 'serif',\n",
    "    'font.serif' : ['Times'],\n",
    "    # axes\n",
    "    'axes.titlesize' : 42,\n",
    "    'axes.labelsize' : 32,\n",
    "    'axes.linewidth' : 2,\n",
    "    # ticks\n",
    "    'xtick.labelsize' : 24,\n",
    "    'ytick.labelsize' : 24,\n",
    "    'xtick.major.size' : 16,\n",
    "    'xtick.minor.size' : 8,\n",
    "    'ytick.major.size' : 16,\n",
    "    'ytick.minor.size' : 8,\n",
    "    'xtick.major.width' : 2,\n",
    "    'xtick.minor.width' : 2,\n",
    "    'ytick.major.width' : 2,\n",
    "    'ytick.minor.width' : 2,\n",
    "    'xtick.direction' : 'in',\n",
    "    'ytick.direction' : 'in',\n",
    "    # markers\n",
    "    'lines.markersize' : 12,\n",
    "    'lines.markeredgewidth' : 3,\n",
    "    'errorbar.capsize' : 8,\n",
    "    'lines.linewidth' : 3,\n",
    "    #'lines.linestyle' : None,\n",
    "    'lines.marker' : None,\n",
    "    'savefig.bbox' : 'tight',\n",
    "    'legend.fontsize' : 24,\n",
    "    #'legend.fontsize': 18,\n",
    "    #'figure.figsize': (15, 5),\n",
    "    #'axes.labelsize': 18,\n",
    "    #'axes.titlesize':18,\n",
    "    #'xtick.labelsize':14,\n",
    "    #'ytick.labelsize':14\n",
    "    'axes.labelsize': 32,\n",
    "    'axes.titlesize' : 32,\n",
    "    'xtick.labelsize' : 25,\n",
    "    'ytick.labelsize' : 25,\n",
    "    'xtick.major.pad' : 10,\n",
    "    'text.latex.unicode': True,\n",
    "}\n",
    "plt.rcParams.update(params)\n",
    "plt.rc('text', usetex=False)\n",
    "\n",
    "def plt_config(title=None, xlim=None, ylim=None, xlabel=None, ylabel=None, colorbar=False, sci=False):\n",
    "    for field in ['title', 'xlim', 'ylim', 'xlabel', 'ylabel']:\n",
    "        if eval(field) != None: getattr(plt, field)(eval(field))\n",
    "    if isinstance(sci, str): plt.ticklabel_format(style='sci', axis=sci, scilimits=(0,0))\n",
    "    if isinstance(colorbar,str): plt.colorbar(label=colorbar)\n",
    "    elif colorbar: plt.colorbar(label = '$Number\\ of\\ Entries$')\n",
    "\n",
    "from contextlib import contextmanager\n",
    "@contextmanager\n",
    "def initiate_plot(dimx=24, dimy=9):\n",
    "    plt.rcParams['figure.figsize'] = (dimx, dimy)\n",
    "    global fig; fig = plt.figure()\n",
    "    yield\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read real data\n",
    "class read_n_concat():   \n",
    "    __version__ = '0.0.3'\n",
    "\n",
    "    def __init__(self):\n",
    "        self.pre_selection = ['True',]\n",
    "        self.pre_selection = '&'.join(self.pre_selection)\n",
    "        self._read_flag = True\n",
    "    \n",
    "    def process(self, indir = '', number_file_cap = 0, number_cup = 1):\n",
    "        self.indir = indir\n",
    "        self.number_file_cap = number_file_cap; self.number_cup = number_cup\n",
    "        \n",
    "        # Record all the file in folder, and put a cap on number of files \n",
    "        if isinstance(self.number_file_cap, int):\n",
    "            file_list = [file for file in os.listdir(self.indir) if 'pkl' in file][:self.number_file_cap]\n",
    "        else:\n",
    "            file_list = self.number_file_cap\n",
    "            \n",
    "        file_list.sort()\n",
    "        \n",
    "        # Process\n",
    "        if self.number_cup > 1:\n",
    "            # Use mulithread processing\n",
    "            with Pool(processes = self.number_cup) as pool:\n",
    "                self.result = list(tqdm(pool.imap(self._process, file_list, 1), total = len(file_list)))\n",
    "        \n",
    "        else:\n",
    "            # Use a for loop to process\n",
    "            self.result = []\n",
    "            with tqdm(total = len(file_list)) as pBar:\n",
    "                for file in file_list:\n",
    "                    pBar.update(1)\n",
    "                    self.result.append(self._process(file))\n",
    "        \n",
    "        self.result, self.errors = zip(*self.result)\n",
    "        \n",
    "        # Concat result\n",
    "        if len(self.result) !=0: self.result = pd.concat(self.result)\n",
    "        \n",
    "        return self.result\n",
    "        \n",
    "    def _process(self, file):\n",
    "        \n",
    "        self._process_list = [self._read_pickle,\n",
    "                              self._reduce_events,\n",
    "                              self._reduce_columns,\n",
    "                             ]\n",
    "        # initiate result in case of process failure\n",
    "        temp = pd.DataFrame(); error = None\n",
    "        rolling_kwarg = dict(file = file)\n",
    "        \n",
    "        for proc in self._process_list:\n",
    "            try:\n",
    "                rolling_kwarg = (proc(**rolling_kwarg))\n",
    "            except Exception as error:\n",
    "                return (temp, (proc.__name__,error))\n",
    "            if rolling_kwarg['finish_flag']:\n",
    "                return (rolling_kwarg['temp'], error)\n",
    "            \n",
    "    def _read_pickle(self, file):\n",
    "        temp = pd.read_pickle(os.path.join(self.indir, file))\n",
    "        return dict(temp = temp, file = file, finish_flag = False)\n",
    "\n",
    "    def _reduce_events(self, temp, file, finish_flag):\n",
    "        clist = ['CutS2Threshold', 'CutInteractionPeaksBiggest', \n",
    "                 '(z_3d_nn < 0)', 'CutDAQVeto', 'CutS1SingleScatter', 'CutFlash',\n",
    "                 'CutBusyCheck', 'CutBusyTypeCheck', 'CutEndOfRunCheck', \n",
    "                ]\n",
    "        self.clist = clist\n",
    "        temp = temp[temp.eval('&'.join(clist))]\n",
    "            \n",
    "        return dict(temp = temp, file = file, finish_flag = False)\n",
    "\n",
    "    def _reduce_columns(self, temp, file, finish_flag):\n",
    "        clist = ['event_number', 'run_number', 'event_time',\n",
    "                 'largest_other_s2', 's1_pattern_fit_bottom_hax', 's1_pattern_fit_hax',\n",
    "                 's2_pattern_fit', 's2_area_fraction_top', 's2_range_50p_area', 's1_area_fraction_top',\n",
    "                 's2_range_90p_area', 's2_range_80p_area',\n",
    "                 'cs1', 'cs2', 's2', 's1', 'drift_time',   \n",
    "                 'x', 'x_3d_nn', 'y', 'y_3d_nn', 'z', 'z_3d_nn',\n",
    "                ] + list(set([c for c in temp.columns if 'Cut' in c]).difference(rnc.clist+['CutAllEnergy']))\n",
    "        clist += [c for c in temp.columns if ('alt' in c) or ('largest_other' in c) or ('s2' in c)]\n",
    "        clist = list(set(clist))\n",
    "        temp = temp.loc[:, clist]\n",
    "        return dict(temp = temp, file = file, finish_flag = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import datetime, time\n",
    "def spread_out_month(EFolder, n):\n",
    "    flist = [f for f in os.listdir(EFolder) if 'pkl' in f]\n",
    "    flist.sort()\n",
    "    dates = np.array([datetime.datetime(2000+int(f[:2]), int(f[2:4]), 1) for f in flist])\n",
    "    d = pd.DataFrame(dict(file=flist, date=dates))\n",
    "\n",
    "    u, indices, counts = np.unique(dates, return_index=True, return_counts=True)\n",
    "    mask = np.concatenate([indices[i]+np.random.choice(range(counts[i]), n) for i in range(len(u))])\n",
    "    d = d.loc[mask,:]; print('Total sampling %d runs' %len(d))\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rnc = read_n_concat()\n",
    "EFolder = '/project2/lgrandi/zhut/data/cleaned/pax_v6.8.0_Rn220_sciencerun1/'\n",
    "d = spread_out_month(EFolder, 16)\n",
    "time.sleep(1)\n",
    "df_rn_1 = rnc.process(indir = EFolder, number_file_cap = d.file.values, number_cup = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rnc = read_n_concat()\n",
    "EFolder = '/project2/lgrandi/zhut/data/compromise/pax_v6.8.0_none_sciencerun1_cpm/'\n",
    "d = spread_out_month(EFolder, 16)\n",
    "time.sleep(1)\n",
    "df_bg_1 = rnc.process(indir = EFolder, number_file_cap = d.file.values, number_cup = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rnc = read_n_concat()\n",
    "EFolder = '/project2/lgrandi/zhut/data/cleaned/pax_v6.8.0_AmBe_sciencerun1/'\n",
    "d = spread_out_month(EFolder, 100)\n",
    "time.sleep(1)\n",
    "df_ab_1 = rnc.process(indir = EFolder, number_file_cap = d.file.values, number_cup = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_rn_1 = df_rn_1[df_rn_1.CutFiducialCylinder1p3T]\n",
    "df_ab_1 = df_ab_1[df_ab_1.CutFiducialZOptimized]\n",
    "df_bg_1 = df_bg_1[df_bg_1.CutFiducialCylinder1p3T]\n",
    "df_cb = pd.concat([df_rn_1, df_bg_1, df_ab_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "import pickle\n",
    "\n",
    "gmix = GaussianMixture(n_components=2, covariance_type='full', max_iter=300, n_init=20)\n",
    "mask = df_cb.eval('(largest_other_s2>0) & (s2>0) & (largest_other_s2_pattern_fit>0) \\\n",
    "       & ((largest_other_s2_delay_main_s1>6e3)|(largest_other_s2_delay_main_s1<0))')\n",
    "trsa = df_cb[mask].sample(10000)\n",
    "X = np.concatenate([np.log10(trsa.loc[:,['largest_other_s2', 'largest_other_s2_pattern_fit', 's2']]),\n",
    "                    trsa.loc[:, ['event_time']]],\n",
    "                   axis=1)\n",
    "gmix = gmix.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with initiate_plot(20, 12):\n",
    "    labels_ = gmix.predict(X)\n",
    "    type0 = trsa[labels_==0]; type1 = trsa[labels_==1]\n",
    "    \n",
    "    plt.scatter(type0.largest_other_s2, type0.largest_other_s2_pattern_fit, edgecolor='none', s=10, color='darkred', alpha=0.3)\n",
    "    plt.scatter(type1.largest_other_s2, type1.largest_other_s2_pattern_fit, edgecolor='none', s=10, color='midnightblue', alpha=0.3)\n",
    "\n",
    "    plt_config(xlim=[10, 1e7], ylim=[10, 1e6], xlabel='Largest Other S2 [pe]', ylabel='Largest Other S2 Pattern Likelihood',\n",
    "              title='Nonparametric Clustering Input Samples')\n",
    "    plt.xscale('log'); plt.yscale('log');\n",
    "    fig.savefig('dbd_s2single_clusteing_input.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('/project2/lgrandi/zhut/s2_single_classifier_gmix.pkl', 'rb') as f:\n",
    "    gmix=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#with open('/project2/lgrandi/zhut/s2_single_classifier_gmix.pkl', 'wb') as f:\n",
    "#    pickle.dump(gmix, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def classify(df):\n",
    "    df['class_number'] = 0\n",
    "    \n",
    "    test = gmix.predict([[1e5, 4e3, 3e5, np.mean(df_cb.event_time)]])[0]\n",
    "    \n",
    "    mask = df.eval('(largest_other_s2>0) & (s2>0) & (largest_other_s2_pattern_fit>0) \\\n",
    "    & ((largest_other_s2_delay_main_s1>6e3)|(largest_other_s2_delay_main_s1<0))')\n",
    "    Y = np.concatenate([np.log10(df.loc[mask,['largest_other_s2', 'largest_other_s2_pattern_fit', 's2']]),\n",
    "                    df.loc[mask, ['event_time']]],\n",
    "                   axis=1)\n",
    "    df.loc[mask,'class_number'] = np.abs(gmix.predict(Y) - (1-test))\n",
    "    return df\n",
    "\n",
    "df_cb = classify(df_cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def s2_single_scatter(x):\n",
    "    y0, y1 = x*0.00832+72.3, x*0.03-109\n",
    "    return y0/(np.exp((x-23300)*5.91e-4)+1)+y1/(np.exp((23300-x)*5.91e-4)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "def plotting_the_rest(right=False):\n",
    "    cmap = plt.get_cmap('autumn_r')\n",
    "    my_cmap = cmap(np.arange(cmap.N))\n",
    "    my_cmap[:,-1] = np.linspace(0, 0.7, cmap.N)\n",
    "    Omy_cmap = ListedColormap(my_cmap)\n",
    "    \n",
    "    cmap = plt.get_cmap('winter_r')\n",
    "    my_cmap = cmap(np.arange(cmap.N))\n",
    "    my_cmap[:,-1] = np.linspace(0, 0.7, cmap.N)\n",
    "    Bmy_cmap = ListedColormap(my_cmap)\n",
    "    \n",
    "    plt.pcolormesh(x_bin_edges, y_bin_edges, nh_a.T, norm=LogNorm(), cmap=plt.get_cmap('viridis'), alpha=.05)\n",
    "    plt.pcolormesh(x_bin_edges, y_bin_edges, nh_0.T, norm=LogNorm(), cmap=Omy_cmap)\n",
    "    plt.pcolormesh(x_bin_edges, y_bin_edges, nh_1.T, norm=LogNorm(), cmap=Bmy_cmap)\n",
    "    \n",
    "    xgrid = x_bin_edges\n",
    "    \n",
    "    \n",
    "    if right:\n",
    "        plt.plot(xgrid, s2_single_scatter(xgrid), 'red', ls='--')\n",
    "        plt_config(xlim=[x_bin_edges[0], x_bin_edges[-1]], ylim=[y_bin_edges[0], y_bin_edges[-1]],\n",
    "              xlabel='S2 [pe]', ylabel='Largest Other S2 [pe]', title='Clustering Result')\n",
    "        return None\n",
    "    plt.plot(xgrid, 0.1064*xgrid*0.65 + 758.80*(xgrid*0.65)**0.05639 - 819.29, 'red', ls='--')\n",
    "    plt_config(xlim=[x_bin_edges[0], x_bin_edges[-1]], ylim=[y_bin_edges[0], y_bin_edges[-1]],\n",
    "              xlabel='Largest Other S2 [pe]', ylabel='Largest Other S2 Pattern Likelihood',\n",
    "              title='Clustering Result Where %d<S2<%d' %(dep/10**0.5, dep*10**0.5))\n",
    "    \n",
    "dfs = ['df_cb_0', 'df_cb_1', 'df_cb_all', 'df_cb']\n",
    "for jx, dep in enumerate(np.logspace(2.5, 6.5, 5)[:]):\n",
    "    ########\n",
    "    with initiate_plot(20, 12):\n",
    "        argx, argy = '{df}.largest_other_s2', '{df}.largest_other_s2_pattern_fit'\n",
    "        x_bin_edges, y_bin_edges = np.logspace(1,6.6,151), np.logspace(1,6.0,151)\n",
    "        \n",
    "        df_cb_all = df_cb[(np.abs(np.log10(df_cb.s2/dep)) < 0.5)]\n",
    "        nh_a,_,_ = np.histogram2d(eval(argx.format(df=dfs[3])), eval(argy.format(df=dfs[3])), bins=[x_bin_edges, y_bin_edges])\n",
    "        \n",
    "        df_cb_0 = df_cb_all[(df_cb_all.class_number==0)]\n",
    "        nh_0,_,_ = np.histogram2d(eval(argx.format(df=dfs[0])), eval(argy.format(df=dfs[0])), bins=[x_bin_edges, y_bin_edges])\n",
    "        df_cb_1 = df_cb_all[(df_cb_all.class_number==1)]\n",
    "        nh_1,_,_ = np.histogram2d(eval(argx.format(df=dfs[1])), eval(argy.format(df=dfs[1])), bins=[x_bin_edges, y_bin_edges])\n",
    "        \n",
    "        ax = fig.add_subplot(111); plotting_the_rest()\n",
    "        plt.xscale('log'); plt.yscale('log')\n",
    "        fig.savefig('dbd_s2single_energy_series_%d.png'%jx, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with initiate_plot(20, 12):       \n",
    "    ####\n",
    "    argx, argy = '{df}.s2', '{df}.largest_other_s2'\n",
    "    x_bin_edges, y_bin_edges = np.logspace(2.1,6.8,151), np.logspace(1,6.8,151)\n",
    "\n",
    "    df_cb_all = df_cb #[(np.abs(np.log10(df_cb.s2/dep)) < 0.5)]\n",
    "    nh_a,_,_ = np.histogram2d(eval(argx.format(df=dfs[3])), eval(argy.format(df=dfs[3])), bins=[x_bin_edges, y_bin_edges])\n",
    "\n",
    "    df_cb_0 = df_cb_all[(df_cb_all.class_number==0)]\n",
    "    nh_0,_,_ = np.histogram2d(eval(argx.format(df=dfs[0])), eval(argy.format(df=dfs[0])), bins=[x_bin_edges, y_bin_edges])\n",
    "    df_cb_1 = df_cb_all[(df_cb_all.class_number==1)]\n",
    "    nh_1,_,_ = np.histogram2d(eval(argx.format(df=dfs[1])), eval(argy.format(df=dfs[1])), bins=[x_bin_edges, y_bin_edges])\n",
    "\n",
    "    ax = fig.add_subplot(111); plotting_the_rest(right=True)\n",
    "    plt.xscale('log'); plt.yscale('log')\n",
    "    fig.savefig('dbd_s2single_original_space.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "splits = [int(datetime.datetime(2017,m,1,0,0).strftime('%s'))*1e9 for m in [3, 4, 5, 6, 7, 8, 9, 10, 11, 12]]\n",
    "splits += [int(datetime.datetime(2018,m,1,0,0).strftime('%s'))*1e9 for m in [1, 2]]\n",
    "df_cb['month_from_start'] = np.digitize(df_cb.event_time, splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "def plotting_the_rest(right=False):\n",
    "    cmap = plt.get_cmap('autumn_r')\n",
    "    my_cmap = cmap(np.arange(cmap.N))\n",
    "    my_cmap[:,-1] = np.linspace(0, 0.7, cmap.N)\n",
    "    Omy_cmap = ListedColormap(my_cmap)\n",
    "    \n",
    "    cmap = plt.get_cmap('winter_r')\n",
    "    my_cmap = cmap(np.arange(cmap.N))\n",
    "    my_cmap[:,-1] = np.linspace(0, 0.7, cmap.N)\n",
    "    Bmy_cmap = ListedColormap(my_cmap)\n",
    "    \n",
    "    plt.pcolormesh(x_bin_edges, y_bin_edges, nh_a.T, norm=LogNorm(), cmap=plt.get_cmap('viridis'), alpha=.05)\n",
    "    plt.pcolormesh(x_bin_edges, y_bin_edges, nh_0.T, norm=LogNorm(), cmap=Omy_cmap)\n",
    "    plt.pcolormesh(x_bin_edges, y_bin_edges, nh_1.T, norm=LogNorm(), cmap=Bmy_cmap)\n",
    "    \n",
    "    xgrid = x_bin_edges\n",
    "    \n",
    "    \n",
    "    if right:\n",
    "        plt.plot(xgrid, s2_single_scatter(xgrid), 'red', ls='--')\n",
    "        plt_config(xlim=[x_bin_edges[0], x_bin_edges[-1]], ylim=[y_bin_edges[0], y_bin_edges[-1]],\n",
    "              xlabel='S2 [pe]', ylabel='Largest Other S2 [pe]', title='Clustering Result')\n",
    "        return None\n",
    "    plt.plot(xgrid, 0.1064*xgrid*0.65 + 758.80*(xgrid*0.65)**0.05639 - 819.29, 'red', ls='--')\n",
    "    plt_config(xlim=[x_bin_edges[0], x_bin_edges[-1]], ylim=[y_bin_edges[0], y_bin_edges[-1]],\n",
    "              xlabel='Largest Other S2 [pe]', ylabel='Largest Other S2 Pattern Likelihood',\n",
    "              title='Clustering Result During %s' % ms[jx]\n",
    "              )\n",
    "\n",
    "dfs = ['df_cb_0', 'df_cb_1', 'df_cb_all', 'df_cb']\n",
    "monthes = df_cb.month_from_start.unique()\n",
    "monthes.sort()\n",
    "ms = ['Feb 2017', 'Mar 2017', 'Apr 2017', 'May 2017', 'Jun 2017', 'Jul 2017', \n",
    "      'Aug 2017', 'Sep 2017', 'Oct 2017', 'Nov 2017', 'Dec 2017', 'Jan 2018', 'Feb 2018']\n",
    "for jx, mo in enumerate(monthes):\n",
    "    ########\n",
    "    with initiate_plot(20, 12):\n",
    "        argx, argy = '{df}.largest_other_s2', '{df}.largest_other_s2_pattern_fit'\n",
    "        x_bin_edges, y_bin_edges = np.logspace(1,6.6,151), np.logspace(1,6.0,151)\n",
    "        \n",
    "        df_cb_all = df_cb[df_cb.month_from_start==mo]\n",
    "        nh_a,_,_ = np.histogram2d(eval(argx.format(df=dfs[3])), eval(argy.format(df=dfs[3])), bins=[x_bin_edges, y_bin_edges])\n",
    "        \n",
    "        df_cb_0 = df_cb_all[(df_cb_all.class_number==0)]\n",
    "        nh_0,_,_ = np.histogram2d(eval(argx.format(df=dfs[0])), eval(argy.format(df=dfs[0])), bins=[x_bin_edges, y_bin_edges])\n",
    "        df_cb_1 = df_cb_all[(df_cb_all.class_number==1)]\n",
    "        nh_1,_,_ = np.histogram2d(eval(argx.format(df=dfs[1])), eval(argy.format(df=dfs[1])), bins=[x_bin_edges, y_bin_edges])\n",
    "        \n",
    "        ax = fig.add_subplot(111); plotting_the_rest()\n",
    "        plt.xscale('log'); plt.yscale('log')\n",
    "        fig.savefig('dbd_s2single_time_series_%d.png'%jx, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
